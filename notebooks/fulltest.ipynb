{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c6db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fair MLP Training on Adult Dataset ===\n",
      "\n",
      "Loading Adult dataset...\n",
      "Dataset loaded with 45222 samples\n",
      "Feature dimensionality: 102\n",
      "\n",
      "Creating MLP model...\n",
      "Model created with 92,417 trainable parameters\n",
      "\n",
      "Initializing SGD training mechanism...\n",
      "\n",
      "==================================================\n",
      "Training on device: cpu\n",
      "Training configuration:\n",
      "  Epochs: 100\n",
      "  Learning rate: 0.01\n",
      "  Batch size: 32\n",
      "  Early stopping patience: 50\n",
      "Dataset splits:\n",
      "  Training samples: 31655\n",
      "  Validation samples: 6783\n",
      "  Test samples: 6784\n",
      "\n",
      "Starting training...\n",
      "Epoch [  1/100] | Train Loss: 5.9257, Train Acc: 0.7692 | Val Loss: 5.9788, Val Acc: 0.7708\n",
      "New best validation loss: 5.9788\n",
      "New best validation loss: 5.9780\n",
      "New best validation loss: 5.9772\n",
      "New best validation loss: 5.9769\n",
      "New best validation loss: 5.9762\n",
      "Epoch [ 10/100] | Train Loss: 5.8841, Train Acc: 0.7751 | Val Loss: 5.9756, Val Acc: 0.7708\n",
      "New best validation loss: 5.9756\n",
      "New best validation loss: 5.9746\n",
      "New best validation loss: 5.9730\n",
      "Epoch [ 20/100] | Train Loss: 5.8863, Train Acc: 0.7751 | Val Loss: 5.9767, Val Acc: 0.7708\n",
      "New best validation loss: 5.9603\n",
      "New best validation loss: 5.9493\n",
      "New best validation loss: 5.9487\n",
      "New best validation loss: 5.9306\n",
      "Epoch [ 30/100] | Train Loss: 5.8948, Train Acc: 0.7751 | Val Loss: 5.9265, Val Acc: 0.7708\n",
      "New best validation loss: 5.9265\n",
      "Epoch [ 40/100] | Train Loss: 5.8851, Train Acc: 0.7751 | Val Loss: 5.9747, Val Acc: 0.7708\n",
      "Epoch [ 50/100] | Train Loss: 5.8850, Train Acc: 0.7751 | Val Loss: 5.9789, Val Acc: 0.7708\n",
      "Epoch [ 60/100] | Train Loss: 5.8859, Train Acc: 0.7751 | Val Loss: 5.9809, Val Acc: 0.7708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 424\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m    422\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(RANDOM_SEED)\n\u001b[0;32m--> 424\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 387\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# 4. Train the model\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m training_results \u001b[38;5;241m=\u001b[39m \u001b[43msgd_mechanism\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madult_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# 5. Display results\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 275\u001b[0m, in \u001b[0;36mSGDMechanism.train\u001b[0;34m(self, model, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Accumulate training statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/fyp/haashim_repo/venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fyp/haashim_repo/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fyp/haashim_repo/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from abc import ABC, abstractmethod\n",
    "from aif360.datasets import AdultDataset as Aif360AdultDataset\n",
    "\n",
    "# Constants\n",
    "RANDOM_SEED = 42\n",
    "ADULT_DATA_FEATURES = 102\n",
    "\n",
    "class AIF360TorchDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    Wraps an AIF360 BinaryLabelDataset as a PyTorch Dataset.\n",
    "    \n",
    "    This wrapper converts AIF360's numpy-based datasets into PyTorch-compatible\n",
    "    format while preserving all the fairness-related metadata like protected attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aif360_dataset, include_protected=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            aif360_dataset: an AIF360 dataset object (e.g. AdultDataset(), COMPASDataset(), etc.)\n",
    "            include_protected: if True, will also expose protected_attributes\n",
    "        \"\"\"\n",
    "        # Extract numpy arrays from AIF360\n",
    "        X = aif360_dataset.features\n",
    "        y = aif360_dataset.labels.ravel()\n",
    "        \n",
    "        # Handle protected attributes if requested\n",
    "        if include_protected:\n",
    "            prot = aif360_dataset.protected_attributes\n",
    "            self.protected_attrs = torch.tensor(prot, dtype=torch.float32)\n",
    "        else:\n",
    "            self.protected_attrs = None\n",
    "        \n",
    "        # Convert to PyTorch tensors with appropriate data types\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        # CRITICAL FIX: Use float32 for labels since we're using BCELoss\n",
    "        self.labels = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return features and labels in the format expected by DataLoader.\n",
    "        For simplicity in training loop, we return a tuple (features, label)\n",
    "        rather than a dictionary.\n",
    "        \"\"\"\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class BaseDataset(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for dataset wrappers that provides a universal interface\n",
    "    for loading, processing and accessing datasets in any format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the dataset by loading the underlying AIF360 data.\"\"\"\n",
    "        self._aif360_dataset = self.load_data()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the AIF360 dataset object.\n",
    "        Must be implemented by subclasses.\n",
    "        \n",
    "        Returns:\n",
    "            AIF360 dataset object\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"load_data() must be implemented in subclasses\")\n",
    "    \n",
    "    def to_torch(self, include_protected=True):\n",
    "        \"\"\"\n",
    "        Converts the AIF360 dataset to PyTorch Datasets with train/val/test splits.\n",
    "        \n",
    "        Args:\n",
    "            include_protected: if True, will also expose protected_attributes\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_dataset, val_dataset, test_dataset)\n",
    "        \"\"\"\n",
    "        converted_dataset = AIF360TorchDataset(self._aif360_dataset, include_protected)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        total_size = len(converted_dataset)\n",
    "        train_size = int(0.7 * total_size)\n",
    "        val_size = int(0.15 * total_size)\n",
    "        test_size = total_size - train_size - val_size\n",
    "        \n",
    "        # Perform the split with fixed random seed for reproducibility\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            converted_dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "class AdultDataset(BaseDataset):  # CRITICAL FIX: Inherit from BaseDataset\n",
    "    \"\"\"\n",
    "    Adult dataset class for loading and preprocessing Adult dataset.\n",
    "    \n",
    "    This class handles the specific configuration needed for the Adult/Census Income\n",
    "    dataset, including proper handling of categorical features and protected attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # This calls BaseDataset.__init__() which calls load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and configure the Adult dataset from AIF360.\n",
    "        \n",
    "        The configuration here follows fairness research best practices:\n",
    "        - 'sex' is the primary protected attribute for fairness evaluation\n",
    "        - Males are considered the privileged group\n",
    "        - Categorical features are properly encoded\n",
    "        - Missing values (marked as '?') are handled by dropping rows\n",
    "        \"\"\"\n",
    "        adult_ds = Aif360AdultDataset(\n",
    "            protected_attribute_names=['sex'],  # Primary protected attribute\n",
    "            privileged_classes=[['Male']],      # Privileged group definition\n",
    "            categorical_features=['workclass', 'education', 'marital-status',\n",
    "                                'occupation', 'relationship', 'race', 'native-country'],\n",
    "            features_to_keep=['age', 'workclass', 'education', 'education-num',\n",
    "                            'marital-status', 'occupation', 'relationship', 'race',\n",
    "                            'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                            'native-country'],\n",
    "            na_values=['?'],  # Handle missing values\n",
    "            custom_preprocessing=lambda df: df.dropna()  # Simple approach: drop missing values\n",
    "        )\n",
    "        \n",
    "        # Verify the expected feature dimensionality\n",
    "        assert adult_ds.features.shape[1] == ADULT_DATA_FEATURES, \\\n",
    "            f\"Expected {ADULT_DATA_FEATURES} features, got {adult_ds.features.shape[1]}\"\n",
    "        \n",
    "        return adult_ds\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for classification tasks.\n",
    "    \n",
    "    This implementation is based on the Fair-Fairness Benchmark repository\n",
    "    and includes some modifications for better integration with our training framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, mlp_layers=[512, 256, 64], p_dropout=0.2, num_classes=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.mlp_layers = [n_features] + mlp_layers\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Create the hidden layers\n",
    "        self.network = nn.ModuleList([\n",
    "            nn.Linear(i, o) for i, o in zip(self.mlp_layers[:-1], self.mlp_layers[1:])\n",
    "        ])\n",
    "        \n",
    "        # Final classification head\n",
    "        self.head = nn.Linear(self.mlp_layers[-1], num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Returns both hidden representation and final prediction to allow for\n",
    "        analysis of intermediate representations (useful for fairness research).\n",
    "        \"\"\"\n",
    "        # Pass through hidden layers with ReLU activation and dropout\n",
    "        for layer in self.network:\n",
    "            x = layer(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.p_dropout, training=self.training)\n",
    "        \n",
    "        # Store hidden representation (useful for fairness analysis)\n",
    "        h = x\n",
    "        \n",
    "        # Final classification layer with sigmoid activation\n",
    "        x = self.head(x)\n",
    "        logits = torch.sigmoid(x)\n",
    "        \n",
    "        return h, logits\n",
    "\n",
    "class SGDMechanism:\n",
    "    \"\"\"\n",
    "    A mechanism that trains a model using SGD, designed for fairness research.\n",
    "    \n",
    "    This implementation includes proper early stopping, comprehensive logging,\n",
    "    and handles the dual-output nature of our MLP model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, model, dataset: BaseDataset, **kwargs):\n",
    "        \"\"\"\n",
    "        Train the model on the dataset using SGD.\n",
    "        \n",
    "        Args:\n",
    "            model: The untrained model to be trained\n",
    "            dataset: The dataset to train on (must be a BaseDataset subclass)\n",
    "            **kwargs: Additional hyperparameters for training\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing training history and metrics\n",
    "        \"\"\"\n",
    "        # Set up device (GPU if available, otherwise CPU)\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device = \"cpu\"\n",
    "        model.to(device)\n",
    "        print(f\"Training on device: {device}\")\n",
    "        \n",
    "        # Extract hyperparameters with sensible defaults\n",
    "        num_epochs = kwargs.get('num_epochs', 100)\n",
    "        learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "        batch_size = kwargs.get('batch_size', 32)\n",
    "        patience = kwargs.get('patience', 10)  # For early stopping\n",
    "        \n",
    "        print(f\"Training configuration:\")\n",
    "        print(f\"  Epochs: {num_epochs}\")\n",
    "        print(f\"  Learning rate: {learning_rate}\")\n",
    "        print(f\"  Batch size: {batch_size}\")\n",
    "        print(f\"  Early stopping patience: {patience}\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset, val_dataset, test_dataset = dataset.to_torch(include_protected=False)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f\"Dataset splits:\")\n",
    "        print(f\"  Training samples: {len(train_dataset)}\")\n",
    "        print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "        print(f\"  Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "        # Set up optimizer and loss function\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        criterion = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
    "        \n",
    "        # Training history tracking\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        print(\"\\nStarting training...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # ===== TRAINING PHASE =====\n",
    "            model.train()  # Enable dropout and batch norm training mode\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                # Move data to device\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                # Ensure labels have correct shape for BCELoss\n",
    "                batch_y = batch_y.view(-1, 1)\n",
    "                \n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass - IMPORTANT: handle dual output from MLP\n",
    "                hidden_repr, outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate training statistics\n",
    "                train_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                train_total += batch_y.size(0)\n",
    "                train_correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            # ===== VALIDATION PHASE =====\n",
    "            model.eval()  # Disable dropout and set batch norm to eval mode\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    batch_y = batch_y.view(-1, 1)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    hidden_repr, outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    \n",
    "                    # Accumulate validation statistics\n",
    "                    val_loss += loss.item()\n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    val_total += batch_y.size(0)\n",
    "                    val_correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            train_acc = train_correct / train_total\n",
    "            val_acc = val_correct / val_total\n",
    "            \n",
    "            # Store history\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f'Epoch [{epoch+1:3d}/{num_epochs}] | '\n",
    "                      f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n",
    "                      f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # CRITICAL FIX: Implement early stopping logic\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save the best model state\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f'New best validation loss: {best_val_loss:.4f}')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Check if we should stop early\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "                print(f'Best validation loss: {best_val_loss:.4f}')\n",
    "                break\n",
    "        \n",
    "        # Restore the best model\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Restored best model weights\")\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'train_losses': train_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "\n",
    "# ===== USAGE EXAMPLE =====\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Demonstrates how to use the corrected framework to train an MLP on the Adult dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Fair MLP Training on Adult Dataset ===\\n\")\n",
    "    \n",
    "    # 1. Load the dataset\n",
    "    print(\"Loading Adult dataset...\")\n",
    "    adult_dataset = AdultDataset()\n",
    "    print(f\"Dataset loaded with {adult_dataset._aif360_dataset.features.shape[0]} samples\")\n",
    "    print(f\"Feature dimensionality: {adult_dataset._aif360_dataset.features.shape[1]}\")\n",
    "    \n",
    "    # 2. Create the model\n",
    "    print(\"\\nCreating MLP model...\")\n",
    "    model = MLP(\n",
    "        n_features=ADULT_DATA_FEATURES,\n",
    "        mlp_layers=[256, 256],  # Following Fair-Fairness Benchmark\n",
    "        p_dropout=0.2,\n",
    "        num_classes=1\n",
    "    )\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model created with {total_params:,} trainable parameters\")\n",
    "    \n",
    "    # 3. Create the training mechanism\n",
    "    print(\"\\nInitializing SGD training mechanism...\")\n",
    "    sgd_mechanism = SGDMechanism()\n",
    "    \n",
    "    # 4. Train the model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    training_results = sgd_mechanism.train(\n",
    "        model=model,\n",
    "        dataset=adult_dataset,\n",
    "        num_epochs=100,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=32,\n",
    "        patience=50\n",
    "    )\n",
    "    \n",
    "    # 5. Display results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    final_train_acc = training_results['train_accuracies'][-1]\n",
    "    final_val_acc = training_results['val_accuracies'][-1]\n",
    "    best_val_loss = training_results['best_val_loss']\n",
    "    \n",
    "    print(f\"Final Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    epochs_trained = len(training_results['train_losses'])\n",
    "    print(f\"Training completed in {epochs_trained} epochs\")\n",
    "    \n",
    "    # The trained model is now ready for fairness evaluation\n",
    "    trained_model = training_results['model']\n",
    "    print(f\"\\nTrained model ready for fairness evaluation!\")\n",
    "    \n",
    "    return training_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    import numpy as np\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f374b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
